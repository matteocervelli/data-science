{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bb8aed1a",
   "metadata": {},
   "source": [
    "# Scaling E-commerce operations in emerging markets: Brazilian SMEs\n",
    "\n",
    "## Introduction\n",
    "\n",
    "### Purpose\n",
    "\n",
    "The purpose of this prooject is to show how mid-sized companies can leverage sales data analytics to overcame growth plateaus and scale. \n",
    "\n",
    "### Context\n",
    "\n",
    "For this project, we will use a dataset containing sales data for a mid-sized Brazilian e-commerce company, with information of 100k orders from 2016 to 2018. This dataset was prepared and shared by Olist on [Kaggle](https://www.kaggle.com/datasets/olistbr/brazilian-ecommerce).\n",
    "\n",
    "### Problems we want to solve\n",
    "\n",
    "For a company, sales are the most important and available data. But analyze this huge datasets could be hard and sometimes data are note cleaned enough.\n",
    "\n",
    "<img src=\"https://images.unsplash.com/photo-1666071083408-a7acb1a87e5f?q=80&w=2940&auto=format&fit=crop&ixlib=rb-4.1.0&ixid=M3wxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8fA%3D%3D\" height=\"250\">\n",
    "\n",
    "In this case study we want to address problems that many business owners and how to analize sales track records effectively is one of the most frequent. \n",
    "\n",
    "Also, how can we use those data to segment and reinforce cross-selling?\n",
    "\n",
    "\n",
    "### About me and why I'm writing this\n",
    "\n",
    "I am a Business Transformation and Scalability Engineer. \n",
    "\n",
    "I help companies to overcome growth thresholds and crisis, like generational transition, market consolidation, and economic downturns, both as a consultant and as a temporary executive. I work directly with business owners, executives and top-management.\n",
    "\n",
    "I have a background as a Management Engineer and an experience in a 2nd to 3rd generation family business, 250 people, 60-80M€ revenue, called [Urania Group](https://urania.group). In Urania, I operated in many field. Accidentally, my first task in Urania Group was to operate as a Sales Manager in the Brazilian market 2014-2016. I have been the CEO of the controlled company Serrall from 2018 to 2024, when I left the governance of the company. \n",
    "\n",
    "Since then, I focused on high-level, strategic consultancy for mid-sized companies. In 2025 I started a new company where I am developing software solutions for mid-sized companies focused on intelligence, growth for owners, executives and top-management.\n",
    "\n",
    "You can find more about me and my work on my [personale website](https://matteocervelli.com/en/about/) and on [LinkedIn](https://www.linkedin.com/in/matteocervelli/).\n",
    "\n",
    "This project is part of the [Google Data Analytics Professional Certificate](https://www.coursera.org/professional-certificates/google-data-analytics) by Google and Coursera. This is my first publicly available project, and I'm sharing it as a case study, portfolio project.\n",
    "\n",
    "<img src=\"https://cdn.adlimen.com/profile/mc-profile-natural.jpg\" width=\"200\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "793c8372",
   "metadata": {},
   "source": [
    "## Ask\n",
    "\n",
    "This case study try to address general questions related to business intelligence, like:\n",
    "\n",
    "- **Time Between Orders**: Which is the mean time occurring between the first and the second order for a returning customer? How does this compare with the following orders? Can we identify a pattern for TBO to predict churn?\n",
    "- What is the mean **churn rate** in market? How can we position companies in churn-rate vs reviews?\n",
    "- How can we **segment** customers?\n",
    "- How do different products relate and support **cross-selling**?\\\n",
    "- **Value Ladder**: Can we find front-end and back-end products just from ordered products?\n",
    "- **Sales prediction**: is it possible from past sales?\n",
    "\n",
    "\n",
    "Particularly for the use case:\n",
    "\n",
    "- What is the seasonality and how can we find different clusters of products based on seasonality?\n",
    "- Which are the most sold products categories?\n",
    "- How can we map product categories in prices per unit vs. average sales?\n",
    "- Is there a correlation between reviews and Time Between Orders?\n",
    "\n",
    "<img src=\"https://images.unsplash.com/photo-1551836022-d5d88e9218df?q=80&w=2070&auto=format&fit=crop&ixlib=rb-4.1.0&ixid=M3wxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8fA%3D%3D\" height=\"250\">\n",
    "\n",
    "Picture by <a href=\"https://unsplash.com/it/@amyhirschi?utm_content=creditCopyText&utm_medium=referral&utm_source=unsplash\">Amy Hirschi</a> on <a href=\"https://unsplash.com/it/foto/donna-in-t-shirt-verde-acqua-che-si-siede-accanto-alla-donna-in-giacca-del-vestito-JaoVGh5aJ3E?utm_content=creditCopyText&utm_medium=referral&utm_source=unsplash\">Unsplash</a>\n",
    "      "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28155909",
   "metadata": {},
   "source": [
    "## Prepare\n",
    "\n",
    "I download the dataset from Kaggle. \n",
    "\n",
    "The dataset has 9 tables:\n",
    "\n",
    "- Core orders dataset\n",
    "- Order items\n",
    "- Order payments\n",
    "- Order reviews\n",
    "- Customers\n",
    "- Brazilian zip codes and coordinates\n",
    "- Products\n",
    "- Sellers\n",
    "- Product category name translations from Brazilian to English\n",
    "\n",
    "![The database schema](https://i.imgur.com/HRhd2Y0.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f479e722",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Needed packages loaded\n",
    "library(tidyverse)\n",
    "library(readr)\n",
    "library(lubridate)\n",
    "library(skimr)\n",
    "library(dplyr)\n",
    "library(janitor)\n",
    "library(ggplot2)\n",
    "library(scales)\n",
    "library(gridExtra)\n",
    "library(treemapify)\n",
    "library(maps)\n",
    "library(ggmap)\n",
    "library(viridis)\n",
    "library(arules)\n",
    "library(arulesViz)\n",
    "library(forecast)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68050228",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Tables imported\n",
    "customers <- read_csv(\"./dataset/olist_customers_dataset.csv\")\n",
    "geolocation <- read_csv(\"./dataset/olist_geolocation_dataset.csv\")\n",
    "order_items <- read_csv(\"./dataset/olist_order_items_dataset.csv\")\n",
    "order_payments <- read_csv(\"./dataset/olist_order_payments_dataset.csv\")\n",
    "order_reviews <- read_csv(\"./dataset/olist_order_reviews_dataset.csv\")\n",
    "orders <- read_csv(\"./dataset/olist_orders_dataset.csv\")\n",
    "products <- read_csv(\"./dataset/olist_products_dataset.csv\")\n",
    "sellers <- read_csv(\"./dataset/olist_sellers_dataset.csv\")\n",
    "category_names_translation <- read_csv(\"./dataset/product_category_name_translation.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c20bd7b6",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "cat(\"-----Snapshot of ORDERS table\\n\")\n",
    "glimpse(orders)\n",
    "\n",
    "cat(\"\\n-----Snapshot of CUSTOMERS table\\n\")\n",
    "glimpse(customers)\n",
    "\n",
    "cat(\"\\n-----Snapshot of ORDER_ITEMS table\\n\")\n",
    "glimpse(order_items)\n",
    "\n",
    "cat(\"\\n-----Snapshot of ORDER_PAYMENTS table\\n\")\n",
    "glimpse(order_payments)\n",
    "\n",
    "cat(\"\\n-----Snapshot of ORDER_REVIEWS table\\n\")\n",
    "glimpse(order_reviews)\n",
    "\n",
    "cat(\"\\n-----Snapshot of CUSTOMERS table\\n\")\n",
    "glimpse(customers)\n",
    "\n",
    "cat(\"\\n-----Snapshot of SELLERS table\\n\")\n",
    "glimpse(sellers)\n",
    "\n",
    "cat(\"\\n-----Snapshot of GEOLOCATION table\\n\")\n",
    "glimpse(geolocation)\n",
    "\n",
    "cat(\"\\n-----Snapshot of PRODUCTS table\\n\")\n",
    "glimpse(products)\n",
    "\n",
    "cat(\"\\n-----Snapshot of CATEGORY_NAME_TRANSLATIONS table\\n\")\n",
    "glimpse(category_names_translation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0888127f",
   "metadata": {},
   "source": [
    "The dataset is well organized, columns names are great and also we are provided with a useful translation from Brazilian to English.\n",
    "\n",
    "We just have to "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c128497",
   "metadata": {},
   "source": [
    "## Process\n",
    "\n",
    "The main table is the ORDERS table, but the most important data is in the ORDER_ITEMS table. We will consider this as the main table from where to choose the columns and joins everything. So we will create a massive table from here. \n",
    "\n",
    "At the end we would do analysis on customers and sellers for quantity, recurrency, ABC, and then on products, as a first touch.\n",
    "\n",
    "We must also rename, while doing the join, the geolocation columns for sellers and customers, to have them duplicated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21a3c9dc",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "tryCatch({\n",
    "  merged_table <- order_items %>%\n",
    "    left_join(orders, by = \"order_id\") %>%\n",
    "    left_join(order_reviews, by = \"order_id\", relationship =\n",
    "  \"many-to-many\") %>%\n",
    "    left_join(order_payments, by = \"order_id\", relationship =\n",
    "  \"many-to-many\") %>%\n",
    "    left_join(customers, by = \"customer_id\") %>%\n",
    "    left_join(geolocation, by = c(\"customer_zip_code_prefix\" = \"geolocation_zip_code_prefix\")) %>%\n",
    "    rename(customer_lat = geolocation_lat, customer_lng = geolocation_lng) %>%\n",
    "    left_join(sellers, by = \"seller_id\") %>%\n",
    "    left_join(geolocation, by = c(\"seller_zip_code_prefix\" = \"geolocation_zip_code_prefix\")) %>%\n",
    "    rename(seller_lat = geolocation_lat, seller_lng = geolocation_lng) %>%\n",
    "    left_join(products, by = \"product_id\") %>%\n",
    "    left_join(category_names_translation, by = \"product_category_name\") %>%\n",
    "    select(-product_category_name) %>%\n",
    "    rename(product_category_name = product_category_name_translation)\n",
    "},\n",
    "error = function(e) {\n",
    "  cat(\"Error: \", e$message, \"\\n\")\n",
    "  return(NULL)\n",
    "}\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45c92bd6",
   "metadata": {},
   "source": [
    "There's and error. The suspect is the geolocation table. \n",
    "\n",
    "Let's check how many rows are after the join with geolocation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82571b1b",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "merged_table_before <- order_items %>%\n",
    "    left_join(orders, by = \"order_id\") %>%\n",
    "    left_join(order_reviews, by = \"order_id\", relationship =\n",
    "  \"many-to-many\") %>%\n",
    "    left_join(order_payments, by = \"order_id\", relationship =\n",
    "  \"many-to-many\") %>%\n",
    "    left_join(customers, by = \"customer_id\")\n",
    "    \n",
    "\n",
    "cat(\"Number of rows before join with customer geolocation:\", nrow(merged_table_before), \"rows\\n\")\n",
    "\n",
    " merged_table_after <-merged_table_before %>% \n",
    "    left_join(geolocation, by = c(\"customer_zip_code_prefix\" = \"geolocation_zip_code_prefix\"), relationship = \"many-to-many\")\n",
    "\n",
    "cat(\"Number of rows after join with customer geolocation:\", nrow(merged_table_after), \"rows\\n\")\n",
    "\n",
    "cat(\"It explodes\", (nrow(merged_table_after) / nrow(merged_table_before)), \"times\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "206eb549",
   "metadata": {},
   "source": [
    "There's and explosion of rows, over 152 times. Let's check why, we suspect that each zip code is mapping many points on the map."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54e552c5",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "geolocation_duplicates <- geolocation %>%\n",
    "    count(geolocation_zip_code_prefix) %>%\n",
    "    arrange(desc(n))\n",
    "\n",
    "  head(geolocation_duplicates, 20)\n",
    "  summary(geolocation_duplicates$n)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9a4a557",
   "metadata": {},
   "source": [
    "Prefix 24220 has 1146 point on the map! And it's only the first result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bf5b29a",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "head(filter(geolocation, geolocation_zip_code_prefix == \"24220\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5c30b43",
   "metadata": {},
   "source": [
    "Well, Niteroi is amazing. But we have to resolve this problem\n",
    "\n",
    "<img src=\"https://dynamic-media-cdn.tripadvisor.com/media/photo-o/11/49/29/a6/vista-noturna-do-parque.jpg?w=1400&h=500&s=1\" height=\"250\"> \n",
    "\n",
    "To have a good enough result, we can use the first point for the geolocation.\n",
    "To have an above average result we can set each zip_code to the average value.\n",
    "To have an EXCELLENT result we can set each zip_code to the median value.\n",
    "\n",
    "Nothing is easier with R."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0fc759b",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "geolocation_median <- geolocation %>%\n",
    "  group_by(geolocation_zip_code_prefix) %>%\n",
    "  summarise(\n",
    "    geolocation_lat = median(geolocation_lat, na.rm = TRUE),\n",
    "    geolocation_lng = median(geolocation_lng, na.rm = TRUE),\n",
    "    geolocation_city = first(geolocation_city),\n",
    "    geolocation_state = first(geolocation_state),\n",
    "    .groups = 'drop'\n",
    "  )\n",
    "\n",
    "cat(\"Number of rows in the original geolocation table:\", nrow(geolocation), \"\\n\")\n",
    "cat(\"Number of rows in the new geolocation_median table:\", nrow(geolocation_median), \"\\n\")\n",
    "\n",
    "cat(\"We have a reduction in the number of rows of\", (nrow(geolocation) / nrow(geolocation_median)), \"times\\n\")\n",
    "\n",
    "cat(\"\\nOriginal geolocation table for Niteroi\\n\")\n",
    "head(filter(geolocation, geolocation_zip_code_prefix == \"24220\"))\n",
    "\n",
    "cat(\"\\nNew geolocation_median table for Niteroi\\n\")\n",
    "head(filter(geolocation_median, geolocation_zip_code_prefix == \"24220\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0725479",
   "metadata": {},
   "source": [
    "That's great. Now let's do the join with the geolocation_median table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1083859e",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "merged_table_all <- order_items %>%\n",
    "    left_join(orders, by = \"order_id\") %>%\n",
    "    rename(order_item_count = order_item_id) %>%\n",
    "    mutate(order_item_id = paste0(order_id, \"-\", order_item_count)) %>%\n",
    "    left_join(order_reviews, by = \"order_id\", relationship =\n",
    "  \"many-to-many\") %>%\n",
    "    left_join(order_payments, by = \"order_id\", relationship =\n",
    "  \"many-to-many\") %>%\n",
    "    left_join(customers, by = \"customer_id\") %>%\n",
    "    left_join(geolocation_median, by = c(\"customer_zip_code_prefix\" = \"geolocation_zip_code_prefix\")) %>%\n",
    "    rename(customer_lat = geolocation_lat, customer_lng = geolocation_lng) %>%\n",
    "    left_join(sellers, by = \"seller_id\") %>%\n",
    "    left_join(geolocation_median, by = c(\"seller_zip_code_prefix\" = \"geolocation_zip_code_prefix\")) %>%\n",
    "    rename(seller_lat = geolocation_lat, seller_lng = geolocation_lng) %>%\n",
    "    left_join(products, by = \"product_id\") %>%\n",
    "    left_join(category_names_translation, by = \"product_category_name\") %>%\n",
    "    select(-product_category_name) %>%\n",
    "    rename(product_category_name = product_category_name_english)\n",
    "\n",
    "# Remove duplicate rows\n",
    "merged_table <- merged_table_all %>%\n",
    "    distinct(order_item_id, .keep_all = TRUE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc040f7f",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Visualize the merged table\n",
    "\n",
    "head(merged_table)\n",
    "\n",
    "cat(\"Number of rows in the merged_table:\", nrow(merged_table), \"\\n\")\n",
    "cat(\"Number of rows in the original order_items table:\", nrow(merged_table), \"\\n\")\n",
    "cat(\"We have \", nrow(merged_table) - nrow(order_items) - 1, \"rows more than the original number of rows, an increase of\", (percent(nrow(merged_table) / nrow(order_items) -1)), \"\\n\\n\")\n",
    "\n",
    "glimpse(merged_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7849fdf5",
   "metadata": {},
   "source": [
    "## Analyze\n",
    "\n",
    "Now that we have a merged table with a right number of rows, we can start to analyze the data following a comprehensive approach that addresses both descriptive insights and strategic business intelligence questions.\n",
    "\n",
    "### Phase 1: Foundation - Descriptive Analytics\n",
    "\n",
    "Let's start with basics:\n",
    "\n",
    "- How many orders we have?\n",
    "- How many customers we have?\n",
    "- How many sellers we have?\n",
    "- How many products we have?\n",
    "\n",
    "Then we can investigate:\n",
    "\n",
    "- Total revenue\n",
    "- Average revenue per order\n",
    "- Average revenue per customer\n",
    "- Average revenue per seller\n",
    "- Average revenue per product\n",
    "\n",
    "Then we will list the top 10 per revenue and quantity:\n",
    "\n",
    "- Top 10 products by revenue\n",
    "- Top 10 categories by revenue\n",
    "- Top 10 customers by revenue\n",
    "- Top 10 sellers by revenue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e8c5e7a",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Counting orders, customers, sellers and products\n",
    "# We use a list to store the results\n",
    "\n",
    "market_overview <- list(\n",
    "  orders = n_distinct(merged_table$order_id),\n",
    "  items = n_distinct(merged_table$order_item_id),\n",
    "  customers = n_distinct(merged_table$customer_unique_id),\n",
    "  sellers = n_distinct(merged_table$seller_id),\n",
    "  products = n_distinct(merged_table$product_id)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a124cdbd",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Calculate separate counts for each order status\n",
    "order_status_breakdown <- merged_table %>%\n",
    "    count(order_status) %>%\n",
    "    arrange(desc(n))\n",
    "\n",
    "i <- 1\n",
    "while(i <= nrow(order_status_breakdown)) {\n",
    "    status_name <- order_status_breakdown$order_status[i]\n",
    "    status_count <- order_status_breakdown$n[i]\n",
    "\n",
    "    var_name <- paste0(status_name, \"_items\")\n",
    "\n",
    "    market_overview[[var_name]] <- status_count\n",
    "\n",
    "    i <- i + 1\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db30c54f",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Calculate total revenue\n",
    "revenue_stats <- merged_table %>%\n",
    "    summarise(\n",
    "      total_revenue = sum(price + freight_value, na.rm = TRUE),\n",
    "      avg_revenue_per_order = total_revenue / market_overview$orders,\n",
    "      avg_revenue_per_customer = total_revenue / market_overview$customers,\n",
    "      avg_revenue_per_seller = total_revenue / market_overview$sellers,\n",
    "      avg_revenue_per_product = total_revenue / market_overview$products,\n",
    "      total_freight_value = sum(freight_value, na.rm = TRUE)\n",
    "    )\n",
    "\n",
    "market_overview[[\"revenues\"]] <- revenue_stats$total_revenue\n",
    "market_overview[[\"freight_value\"]] <- revenue_stats$total_freight_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52d62bb0",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Show results\n",
    "cat(\"=== MARKET OVERVIEW ===\\n\")\n",
    "cat(\"Orders:\", format(market_overview$orders, big.mark = \",\"), \"\\n\")\n",
    "\n",
    "cat(\"\\nItems:\", format(market_overview$items, big.mark = \",\"), \"\\n\")\n",
    "cat(\"├── Delivered items:\", format(market_overview$delivered_items, big.mark = \",\"), \"\\n\")\n",
    "cat(\"├── Shipped items:\", format(market_overview$shipped_items, big.mark = \",\"), \"\\n\")\n",
    "cat(\"├── Invoiced items:\", format(market_overview$invoiced_items, big.mark = \",\"), \"\\n\")\n",
    "cat(\"├── Processing items:\", format(market_overview$processing_items, big.mark = \",\"), \"\\n\")\n",
    "cat(\"├── Approved items:\", format(market_overview$approved_items, big.mark = \",\"), \"\\n\")\n",
    "cat(\"├── Cancelled items:\", format(market_overview$cancelled_items, big.mark = \",\"), \"\\n\")\n",
    "cat(\"└── Unavailable items:\", format(market_overview$unavailable_items, big.mark = \",\"), \"\\n\\n\")\n",
    "cat(\"Customers:\", format(market_overview$customers, big.mark = \",\"), \"\\n\")\n",
    "cat(\"Sellers:\", format(market_overview$sellers, big.mark = \",\"), \"\\n\")\n",
    "cat(\"Products:\", format(market_overview$products, big.mark = \",\"), \"\\n\")\n",
    "\n",
    "cat(\"\\n\\n=== REVENUE STATS ===\\n\")\n",
    "cat(\"Total revenue: R$\", format(revenue_stats$total_revenue, big.mark = \",\"), \"\\n\")\n",
    "cat(\"\\nTotal freight value: R$\", format(revenue_stats$total_freight_value, big.mark = \",\"), \"\\n\")\n",
    "\n",
    "cat(\"\\nAverage revenue:\\n\")\n",
    "\n",
    "\n",
    "\n",
    "cat(\"├── Average revenue per order: R$\", format(round(revenue_stats$avg_revenue_per_order), big.mark = \",\"), \"\\n\")\n",
    "cat(\"├── Average revenue per customer: R$\", format(round(revenue_stats$avg_revenue_per_customer), big.mark = \",\"), \"\\n\")\n",
    "cat(\"├── Average revenue per seller: R$\", format(round(revenue_stats$avg_revenue_per_seller), big.mark = \",\"), \"\\n\")\n",
    "cat(\"└── Average revenue per product: R$\", format(round(revenue_stats$avg_revenue_per_product), big.mark = \",\"), \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5cf513d",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d31a0509",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Top 5 list\n",
    "top_element_per_revenue <- list(\n",
    "    products = merged_table %>%\n",
    "        group_by(product_id, product_category_name) %>%\n",
    "        summarise(revenue = sum(price + freight_value, na.rm = TRUE), .groups = 'drop') %>%\n",
    "        arrange(desc(revenue)),\n",
    "    categories = merged_table %>%\n",
    "        group_by(product_category_name) %>%\n",
    "        summarise(revenue = sum(price + freight_value, na.rm = TRUE), .groups = 'drop') %>%\n",
    "        arrange(desc(revenue)),\n",
    "    customers = merged_table %>%\n",
    "        group_by(customer_id, customer_city, customer_state) %>%\n",
    "        summarise(revenue = sum(price + freight_value, na.rm = TRUE), .groups = 'drop') %>%\n",
    "        arrange(desc(revenue)),\n",
    "    sellers = merged_table %>%\n",
    "        group_by(seller_id, seller_city, seller_state) %>%\n",
    "        summarise(revenue = sum(price + freight_value, na.rm = TRUE), .groups = 'drop') %>%\n",
    "        arrange(desc(revenue)),\n",
    "    states = merged_table %>%\n",
    "        group_by(customer_state) %>%\n",
    "        summarise(revenue = sum(price + freight_value, na.rm = TRUE), .groups = 'drop') %>%\n",
    "        arrange(desc(revenue))\n",
    ")\n",
    "\n",
    "head(top_element_per_revenue$categories, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1e820a9",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Top 5 graphics creation\n",
    "\n",
    "categories_treemap_plot <- ggplot(top_element_per_revenue$categories, aes(area = revenue, fill = product_category_name)) +\n",
    "    geom_treemap() +\n",
    "    geom_treemap_text(aes(label = product_category_name), colour = \"white\", place = \"centre\") +\n",
    "    labs(title = \"Top 5 Categories\") +\n",
    "    theme(legend.position = \"none\")\n",
    "\n",
    "states_treemap_plot <- ggplot(top_element_per_revenue$states, aes(area = revenue, fill = customer_state)) +\n",
    "    geom_treemap() +\n",
    "    geom_treemap_text(aes(label = customer_state), colour = \"white\", place = \"centre\") +\n",
    "    labs(title = \"Top 5 States\") +\n",
    "    theme(legend.position = \"none\")\n",
    "\n",
    "categories_treemap_plot\n",
    "states_treemap_plot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83343bfb",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Show top 10 products, customers and sellers\n",
    "head(top_element_per_revenue$products, 10)\n",
    "head(top_element_per_revenue$customers, 10)\n",
    "head(top_element_per_revenue$sellers, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c587618",
   "metadata": {},
   "source": [
    "### Phase 2: Market Structure Analysis\n",
    "\n",
    "We can analyze the seasonality of the sales and identify seasonal clusters of products.\n",
    "\n",
    "Then we analyze the ABC of the products for inventory optimization.\n",
    "\n",
    "We can analyze some location-based graphs and the Top 10 locations with total revenues, total quantity and average revenue per order. We can do the same for all the states."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91a898ba",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "## Prepare monthly sales data summarization\n",
    "\n",
    "monthly_sales <- merged_table %>%\n",
    "    mutate(order_month = floor_date(as.Date(order_purchase_timestamp),\"month\")) %>%\n",
    "    group_by(order_month) %>%\n",
    "    summarise(\n",
    "        total_revenue = sum(price, na.rm = TRUE),\n",
    "        total_freight_value = sum(freight_value, na.rm = TRUE),\n",
    "        total_orders = n_distinct(order_id),\n",
    "        .groups = 'drop'\n",
    "    ) %>%\n",
    "arrange(order_month)\n",
    "\n",
    "monthly_sales <- monthly_sales %>%\n",
    "    pivot_longer(cols = c(total_revenue, total_freight_value), names_to = \"metric\", values_to = \"value\") %>%\n",
    "    mutate(metric = case_when(\n",
    "        metric == \"total_revenue\" ~ \"Revenue\",\n",
    "        metric == \"total_freight_value\" ~ \"Freight Value\",\n",
    "        TRUE ~ metric\n",
    "    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaee6b9c",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Line chart for monthly trends\n",
    "monthly_trends_plot <- ggplot(monthly_sales, aes(x = order_month, y=value, color=metric)) +\n",
    "    geom_line(linewidth = 1.2) +\n",
    "    geom_point(size = 1.2) +\n",
    "    scale_color_manual(values = c(\"Revenue\" = \"steelblue\", \"Freight Value\" = \"darkgreen\")) +\n",
    "    labs(\n",
    "        title = \"Monthly Sales Trends - Seasonality Analysis\",\n",
    "        x = \"Month\",\n",
    "        y = \"Total Revenue (R$)\",\n",
    "        color = \"Type\"\n",
    "    ) +\n",
    "    theme_minimal() +\n",
    "    scale_y_continuous(labels = scales::comma) +\n",
    "    theme(axis.text.x = element_text(angle = 45, hjust = 1))\n",
    "monthly_trends_plot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eca64b5",
   "metadata": {},
   "source": [
    "As we can see, there's not apparent seasonality in the sales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06af413b",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# ABC Analysis of product categories\n",
    "\n",
    "abc_analysis <- merged_table %>%\n",
    "  group_by(product_category_name) %>%\n",
    "  summarise(revenue = sum(price + freight_value, na.rm = TRUE), .groups = 'drop') %>%\n",
    "  arrange(desc(revenue)) %>%\n",
    "  mutate(\n",
    "    cumulative_revenue = cumsum(revenue),\n",
    "    total_revenue = sum(revenue),\n",
    "    cumulative_percentage = cumulative_revenue / total_revenue * 100,\n",
    "    rank = row_number()\n",
    "  ) %>%\n",
    "  mutate(\n",
    "    abc_class = case_when(\n",
    "    cumulative_percentage <= 80 ~ \"A\",\n",
    "    cumulative_percentage <= 95 ~ \"B\",\n",
    "    TRUE ~ \"C\"\n",
    "    )\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a172af6d",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Pareto chart\n",
    "pareto_plot <- ggplot(abc_analysis, aes(x = rank)) +\n",
    "  geom_col(aes(y = revenue, fill = abc_class), alpha = 0.7) +\n",
    "  geom_line(aes(y = cumulative_percentage * max(revenue) / 100), color = \"darkred\", size = 1.2) +\n",
    "  scale_y_continuous(\n",
    "    name = \"Revenue (R$)\",\n",
    "    labels = scales::comma,\n",
    "    sec.axis = sec_axis(~ . * 100 / max(abc_analysis$revenue), name = \"Cumulative %\")\n",
    "  ) +\n",
    "  scale_fill_manual(values = c(\"A\" = \"darkgreen\", \"B\" = \"orange\", \"C\" = \"red\")) +\n",
    "  labs(\n",
    "    title = \"ABC Analysis - Product Category Revenue Pareto Chart\",\n",
    "    x = \"Product Rank\",\n",
    "    fill = \"ABC Class\"\n",
    "  ) +\n",
    "  theme_minimal()\n",
    "\n",
    "# Pareto crossover point\n",
    "pareto_crossover <- abc_analysis %>% \n",
    "  filter(abc_class == \"A\") %>% \n",
    "  tail(1)\n",
    "\n",
    "pareto_crossover_x <- as.numeric(round(pareto_crossover[ , \"rank\"] / nrow(abc_analysis) * 100, 0))\n",
    "pareto_crossover_y <- as.numeric(round(pareto_crossover[ , \"cumulative_percentage\"], 0))\n",
    "\n",
    "annotation_x <- (pareto_crossover_x + 40) * max(abc_analysis$rank) / 100\n",
    "annotation_y <- pareto_crossover_y * max(abc_analysis$revenue) / 100\n",
    "\n",
    "pareto_plot <- pareto_plot +\n",
    "    annotate(\"text\", x = annotation_x, y = annotation_y, label = paste(\"\\nPareto crossover point: \", \"\\n\", pareto_crossover_x, \"% of products, \", pareto_crossover_y, \"% of revenue\\n\", sep = \"\"), color = \"darkred\", size = 5, fontface = \"bold\")\n",
    "\n",
    "pareto_plot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb7c9882",
   "metadata": {},
   "source": [
    "In categories, there is a standard Pareto classification. Nothing particulare to signal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93ceecb6",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "location_metrics <- merged_table %>%\n",
    "  group_by(customer_state) %>%\n",
    "  summarise(\n",
    "    total_revenue = sum(price + freight_value, na.rm = TRUE),\n",
    "    total_quantity = n(),\n",
    "    avg_order_value = total_revenue / n_distinct(order_id),\n",
    "    .groups = 'drop'\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18559806",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Geographical heatmap\n",
    "\n",
    "# Get the map data\n",
    "brazil_map <- map_data(\"world\") %>% \n",
    "  filter(region == \"Brazil\")\n",
    "\n",
    "# Merge the map data with the location metrics\n",
    "zip_revenue_heatmap <- merged_table %>%\n",
    "  group_by(customer_zip_code_prefix, customer_lat, customer_lng) %>%\n",
    "  summarise(\n",
    "    revenue = sum(price + freight_value, na.rm = TRUE),\n",
    "    orders = n_distinct(order_id),\n",
    "    customers = n_distinct(customer_id),\n",
    "    .groups = 'drop'\n",
    "  ) %>%\n",
    "  filter(!is.na(customer_lat), !is.na(customer_lng), revenue > 0)\n",
    "\n",
    "# Remove outliers\n",
    "revenue_threshold <- quantile(zip_revenue_heatmap$revenue, 0.99)\n",
    "zip_filtered <- zip_revenue_heatmap %>%\n",
    "  filter(revenue <= revenue_threshold)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8758b40",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "brazil_states_heatmap <- ggplot() +\n",
    "  geom_polygon(\n",
    "    data = brazil_map, \n",
    "    aes(x = long, y = lat, group = group), \n",
    "    fill = \"darkseagreen4\", color = \"darkseagreen4\", size = 0.1\n",
    "  ) +\n",
    "  geom_point(\n",
    "    data = zip_filtered,\n",
    "    aes(x = customer_lng, y = customer_lat, color = revenue, size = orders), alpha = 0.7\n",
    "  ) +\n",
    "  scale_color_gradient(\n",
    "    low = \"#ffff00\",\n",
    "    high = \"#ff0000\",\n",
    "    name = \"Revenue (R$)\",\n",
    "    labels = scales::comma_format(),\n",
    "    trans = \"sqrt\",\n",
    "    guide = guide_colorbar(\n",
    "      barwidth = 1.2,\n",
    "      barheight = 8,\n",
    "      title.position = \"top\"\n",
    "    )\n",
    "  ) +\n",
    "  scale_size_continuous(\n",
    "    range = c(0.2, 3),\n",
    "    name = \"Orders\",\n",
    "    labels = scales::comma_format(),\n",
    "    guide = guide_legend(\n",
    "      override.aes = list(alpha = 1),\n",
    "      title.position = \"top\",\n",
    "      title.hjust = 0.5\n",
    "    )\n",
    "  ) +\n",
    "  coord_fixed(xlim = c(-72, -35), ylim = c(-33, 3)) +\n",
    "  labs(\n",
    "    title = \"Brazil E-commerce Revenue Distribution by State\",\n",
    "    subtitle = paste(\"Revenue heatmap by ZIP code | Outliers >\", scales::comma(revenue_threshold), \"R$ excluded\"),\n",
    "    caption = \"Source: Brazilian E-commerce Dataset | Visualization: Revenue Analysis\",\n",
    "    x = \"Longitude\",\n",
    "    y = \"Latitude\"\n",
    "  ) +\n",
    "  theme_void() +\n",
    "  theme(\n",
    "    plot.title = element_text(hjust = 0.5, color = \"black\", size = 18, face = \"bold\", margin = margin(10,0,5,0)),\n",
    "    plot.subtitle = element_text(hjust = 0.5, color = \"grey10\", size = 12, margin = margin(0,0,15,0)),\n",
    "    plot.caption = element_text(hjust = 1, color = \"grey20\", size = 9, margin = margin(15,0,5,0)),\n",
    "    panel.background = element_rect(fill = \"gray60\"),\n",
    "    plot.background = element_rect(fill = \"white\"),\n",
    "    legend.position = \"right\",\n",
    "    legend.text = element_text(color = \"black\", size = 10),\n",
    "    legend.title = element_text(color = \"black\", size = 11, face = \"bold\"),\n",
    "    legend.background = element_rect(fill = \"white\"),\n",
    "    plot.margin = margin(20, 20, 20, 20)\n",
    "  )\n",
    "\n",
    "main_states <- data.frame(\n",
    "  state = c(\"SP\", \"RJ\", \"MG\", \"RS\", \"PR\", \"SC\", \"GO\", \"BA\", \"PE\", \"CE\"),\n",
    "  lng = c(-46.6, -43.2, -44.0, -51.2, -49.3, -48.5, -49.3, -38.5, -34.9, -38.5),\n",
    "  lat = c(-23.5, -22.9, -19.9, -30.0, -25.4, -27.6, -16.7, -12.0, -8.0, -3.7)\n",
    ")\n",
    "\n",
    "brazil_labeled_heatmap <- brazil_states_heatmap +\n",
    "  geom_text(data = main_states,\n",
    "            aes(x = lng, y = lat, label = state),\n",
    "            color = \"black\", size = 5, fontface = \"bold\",\n",
    "            alpha = 0.9)\n",
    "\n",
    "brazil_labeled_heatmap"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcd529bb",
   "metadata": {},
   "source": [
    "Looking the map we can see the geographical concentration of the sales, that's crucial on a logistic point of view.\n",
    "\n",
    "Last, we want to draw a positioning matrix, to divide states into four strategic quadrants based on volume and value metrics:\n",
    "\n",
    "- **HIGH Volume + HIGH Value** (top-right): Premium states with many orders and high value\n",
    "- **LOW Volume + HIGH Value** (top-left): Niche states with few orders but high value  \n",
    "- **HIGH Volume + LOW Value** (bottom-right): Mass market states with many orders but low value\n",
    "- **LOW Volume + LOW Value** (bottom-left): Development opportunity states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60a0e232",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Prepare data for positioning matrix\n",
    "state_positioning_data <- merged_table %>%\n",
    "  group_by(customer_state) %>%\n",
    "  summarise(\n",
    "    total_revenue = sum(price + freight_value, na.rm = TRUE),\n",
    "    total_orders = n_distinct(order_item_id),\n",
    "    customers = n_distinct(customer_unique_id),\n",
    "    avg_revenue_per_customer = total_revenue / customers,\n",
    "    avg_revenue_per_order = total_revenue / total_orders,\n",
    "    .groups = 'drop'\n",
    "  ) %>%\n",
    "  mutate(\n",
    "\n",
    "  ) %>%\n",
    "  filter(!is.na(customer_state)) %>%\n",
    "  arrange(desc(avg_revenue_per_customer))\n",
    "\n",
    "View(state_positioning_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de4ec78e",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Create quadrant reference points\n",
    "median_revenue_per_customer <- median(state_positioning_data$avg_revenue_per_customer)\n",
    "median_revenue_per_order <- median(state_positioning_data$avg_revenue_per_order)\n",
    "\n",
    "# Set the limits for the axes\n",
    "min_revenue_per_customer <-\n",
    "min(state_positioning_data$avg_revenue_per_customer) * 0.9\n",
    "min_revenue_per_order <- min(state_positioning_data$avg_revenue_per_order) * 0.9\n",
    "max_revenue_per_customer <-\n",
    "max(state_positioning_data$avg_revenue_per_customer) * 1.1\n",
    "max_revenue_per_order <- max(state_positioning_data$avg_revenue_per_order) * 1.05\n",
    "\n",
    "# State positioning matrix visualization\n",
    "state_positioning_matrix <- ggplot(state_positioning_data, \n",
    "                                  aes(x = avg_revenue_per_customer, y = avg_revenue_per_order)) +\n",
    "  \n",
    "  # Quadrant reference lines\n",
    "  geom_vline(xintercept = median_revenue_per_customer, color = \"grey60\", linetype = \"dashed\", alpha = 0.7) +\n",
    "  geom_hline(yintercept = median_revenue_per_order, color = \"grey60\", linetype = \"dashed\", alpha = 0.7) +\n",
    "  \n",
    "  # Data points colored by total revenue, sized by customer count\n",
    "  geom_point(aes(color = total_revenue, size = total_revenue), alpha = 0.8) +\n",
    "  \n",
    "  # State labels\n",
    "  geom_text(aes(label = customer_state), vjust = -0.8, hjust = 0.5, size = 3.5, fontface = \"bold\") +\n",
    "  \n",
    "  # Color and size scales\n",
    "  scale_color_gradient2(\n",
    "    low = \"#2166ac\",     # Blue for low revenue\n",
    "    mid = \"#f7f7f7\",     # Grey for medium revenue  \n",
    "    high = \"#d73027\",    # Red for high revenue\n",
    "    midpoint = median(state_positioning_data$total_revenue),\n",
    "    name = \"Total Revenue (R$)\",\n",
    "    labels = scales::comma_format(),\n",
    "    guide = guide_colorbar(barwidth = 1.2, barheight = 8)\n",
    "  ) +\n",
    "  scale_size_continuous(\n",
    "    range = c(3, 12), \n",
    "    name = \"Total Revenue (R$)\",\n",
    "    labels = scales::comma_format(),\n",
    "    guide = guide_legend(override.aes = list(alpha = 1))\n",
    "  ) +\n",
    "  \n",
    "  # Quadrant annotations\n",
    "  annotate(\n",
    "    \"text\",\n",
    "    x = min_revenue_per_customer + (median_revenue_per_customer - min_revenue_per_customer) * 0.6,\n",
    "    y = median_revenue_per_order + (max_revenue_per_order - median_revenue_per_order) * 0.9,\n",
    "    label = \"LOW customer\\nHIGH order\", \n",
    "    size = 4, \n",
    "    color = \"darkred\",\n",
    "    fontface = \"bold\", \n",
    "    alpha = 0.7) +\n",
    "\n",
    "  annotate(\"text\",\n",
    "    x = median_revenue_per_customer + (max_revenue_per_customer - median_revenue_per_customer) * 0.4,\n",
    "    y = median_revenue_per_order + (max_revenue_per_order - median_revenue_per_order) * 0.9,\n",
    "    label = \"HIGH customer\\nHIGH order\", \n",
    "    size = 4, \n",
    "    color = \"darkgreen\", \n",
    "    fontface = \"bold\", \n",
    "    alpha = 0.7) +\n",
    "\n",
    "  annotate(\"text\" ,\n",
    "    x = min_revenue_per_customer + (median_revenue_per_customer - min_revenue_per_customer) * 0.6,\n",
    "    y = min_revenue_per_order + (median_revenue_per_order - min_revenue_per_order) * 0.1,\n",
    "    label = \"LOW customer\\nLOW order\", \n",
    "    size = 4, \n",
    "    color = \"grey40\",\n",
    "    fontface = \"bold\", \n",
    "    alpha = 0.7) +\n",
    "\n",
    "  annotate(\"text\",\n",
    "    x = median_revenue_per_customer + (max_revenue_per_customer - median_revenue_per_customer) * 0.4,\n",
    "    y = min_revenue_per_order + (median_revenue_per_order - min_revenue_per_order) * 0.1,\n",
    "    label = \"HIGH customer\\nLOW order\", \n",
    "    size = 4, \n",
    "    color = \"orange\",\n",
    "    fontface = \"bold\", \n",
    "    alpha = 0.7) +\n",
    "\n",
    "  # Axis formatting\n",
    "  scale_x_continuous(labels = scales::dollar_format(prefix = \"R$ \"), limits = c(min_revenue_per_customer, max_revenue_per_customer)) +\n",
    "  scale_y_continuous(labels = scales::dollar_format(prefix = \"R$ \"), limits = c(min_revenue_per_order, max_revenue_per_order)) +\n",
    "  \n",
    "  # Labels and theme\n",
    "  labs(\n",
    "    title = \"State Positioning Matrix: Business Performance Analysis\",\n",
    "    subtitle = \"Revenue per Order vs Revenue per Customer | Bubble size = Customer count\",\n",
    "    x = \"Average Revenue per Customer\",\n",
    "    y = \"Average Revenue per Order\",\n",
    "    caption = \"Source: Brazilian E-commerce Dataset\\nQuadrants based on median values\"\n",
    "  ) +\n",
    "  theme_minimal() +\n",
    "  theme(\n",
    "    plot.title = element_text(hjust = 0.5, size = 16, face = \"bold\"),\n",
    "    plot.subtitle = element_text(hjust = 0.5, size = 12, color = \"grey30\"),\n",
    "    plot.caption = element_text(hjust = 1, size = 9, color = \"grey50\"),\n",
    "    legend.position = \"right\",\n",
    "    axis.title = element_text(face = \"bold\")\n",
    "  )\n",
    "\n",
    "# Display the matrix\n",
    "print(state_positioning_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e4bec12",
   "metadata": {},
   "source": [
    "This is interesting. We can see a clear correlation between the average revenue per customer and the average revenue per order."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e0e3063",
   "metadata": {},
   "source": [
    "## Phase 3: Strategic Business Intelligence Analysis\n",
    "\n",
    "It's time to go deeper in the analysis with some Business Intelligence findings, on:\n",
    "\n",
    "- Customer Lifecycle & Retention\n",
    "- Product Strategy & Revenue Optimization\n",
    "- Predictive Analytics\n",
    "- Performance Correlations\n",
    "\n",
    "### Customer Lifecycle & Retention\n",
    "\n",
    "#### Time Between Orders (TBO)\n",
    "\n",
    "Mean time between first and second order for returning customers, and pattern identification for churn prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dd4bca0",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "customer_orders <- merged_table %>%\n",
    "  filter(!is.na(order_purchase_timestamp)) %>%\n",
    "  group_by(customer_unique_id) %>%\n",
    "  arrange(order_purchase_timestamp) %>%\n",
    "  summarise(\n",
    "    first_order = min(as.Date(order_purchase_timestamp)),\n",
    "    last_order = max(as.Date(order_purchase_timestamp)),\n",
    "    total_orders = n_distinct(order_id),\n",
    "    unique_order_dates = n_distinct(as.Date(order_purchase_timestamp)),\n",
    "    .groups = 'drop'\n",
    "  ) %>%\n",
    "  filter(total_orders > 1, unique_order_dates > 1) # Need multiple distinct order dates\n",
    "\n",
    "print(\"=== CUSTOMER ORDERS DEBUG ===\")\n",
    "print(paste(\"Total customers:\", n_distinct(merged_table$customer_unique_id)))\n",
    "print(paste(\"Customers with multiple orders:\", nrow(customer_orders)))\n",
    "\n",
    "# Only proceed if we have returning customers\n",
    "if(nrow(customer_orders) > 0) {\n",
    "  \n",
    "  # Calculate actual time between orders\n",
    "  customer_tbo <- merged_table %>%\n",
    "    filter(customer_unique_id %in% customer_orders$customer_unique_id) %>%\n",
    "    select(customer_unique_id, order_id, order_purchase_timestamp) %>%\n",
    "    distinct() %>%\n",
    "    arrange(customer_unique_id, order_purchase_timestamp) %>%\n",
    "    group_by(customer_unique_id) %>%\n",
    "    mutate(\n",
    "      order_date = as.Date(order_purchase_timestamp),\n",
    "      prev_order_date = lag(order_date),\n",
    "      days_between = as.numeric(order_date - prev_order_date)\n",
    "    ) %>%\n",
    "    filter(!is.na(days_between)) %>%\n",
    "    ungroup()\n",
    "  \n",
    "} else {\n",
    "  print(\"=== NO RETURNING CUSTOMERS FOUND ===\")\n",
    "  print(\"This dataset appears to have mostly one-time customers\")\n",
    "  \n",
    "  # Alternative: Show order frequency distribution\n",
    "  order_frequency <- merged_table %>%\n",
    "    group_by(customer_unique_id) %>%\n",
    "    summarise(orders = n_distinct(order_id), .groups = 'drop') %>%\n",
    "    count(orders, name = \"customers\")\n",
    "  \n",
    "  print(\"Order frequency distribution:\")\n",
    "  print(order_frequency)\n",
    "  \n",
    "  customer_tbo <- data.frame() # Empty dataframe\n",
    "}\n",
    "\n",
    "# TBO Distribution Analysis - Only if we have data\n",
    "if(nrow(customer_tbo) > 0) {\n",
    "  \n",
    "  tbo_summary <- customer_tbo %>%\n",
    "    summarise(\n",
    "      mean_tbo = mean(days_between, na.rm = TRUE),\n",
    "      median_tbo = median(days_between, na.rm = TRUE),\n",
    "      sd_tbo = sd(days_between, na.rm = TRUE),\n",
    "      returning_customers = n_distinct(customer_unique_id),\n",
    "      total_intervals = n(),\n",
    "      .groups = 'drop'\n",
    "    )\n",
    "  \n",
    "  print(\"=== TIME BETWEEN ORDERS ANALYSIS ===\")\n",
    "  print(tbo_summary)\n",
    "  \n",
    "  # TBO histogram\n",
    "  tbo_histogram <- ggplot(customer_tbo, aes(x = days_between)) +\n",
    "    geom_histogram(bins = 50, fill = \"steelblue\", alpha = 0.7, color = \"white\") +\n",
    "    geom_vline(aes(xintercept = mean(days_between)), color = \"red\", linetype = \"dashed\", size = 1) +\n",
    "    labs(\n",
    "      title = \"Time Between Orders Distribution\",\n",
    "      subtitle = paste(\"Mean:\", round(tbo_summary$mean_tbo, 1), \"days -\", tbo_summary$returning_customers, \"returning customers\"),\n",
    "      x = \"Days Between Orders\",\n",
    "      y = \"Frequency\"\n",
    "    ) +\n",
    "    theme_minimal() +\n",
    "    scale_x_continuous(limits = c(0, 365))\n",
    "  \n",
    "  print(tbo_histogram)\n",
    "  \n",
    "} else {\n",
    "  print(\"Skipping TBO histogram - no returning customers found\")\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecc633ae",
   "metadata": {},
   "source": [
    "#### Churn Rate Analysis\n",
    "\n",
    "Market churn rate analysis and positioning companies in churn-rate vs reviews matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "facb2cef",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Define churn threshold (customers who haven't ordered in last 6 months)\n",
    "analysis_date <- max(as.Date(merged_table$order_purchase_timestamp), na.rm = TRUE)\n",
    "churn_threshold_days <- 180\n",
    "\n",
    "# Customer churn analysis\n",
    "customer_churn <- merged_table %>%\n",
    "  group_by(customer_unique_id) %>%\n",
    "  summarise(\n",
    "    last_order_date = max(as.Date(order_purchase_timestamp), na.rm = TRUE),\n",
    "    total_orders = n_distinct(order_id),\n",
    "    total_revenue = sum(price + freight_value, na.rm = TRUE),\n",
    "    avg_review_score = mean(review_score, na.rm = TRUE),\n",
    "    .groups = 'drop'\n",
    "  ) %>%\n",
    "  mutate(\n",
    "    days_since_last_order = as.numeric(analysis_date - last_order_date),\n",
    "    is_churned = ifelse(days_since_last_order > churn_threshold_days, 1, 0),\n",
    "    customer_segment = case_when(\n",
    "      total_orders == 1 ~ \"One-time\",\n",
    "      total_orders <= 3 ~ \"Occasional\", \n",
    "      TRUE ~ \"Frequent\"\n",
    "    )\n",
    "  )\n",
    "\n",
    "# Churn rate by segment\n",
    "churn_rates <- customer_churn %>%\n",
    "  group_by(customer_segment) %>%\n",
    "  summarise(\n",
    "    customers = n(),\n",
    "    churned_customers = sum(is_churned),\n",
    "    churn_rate = churned_customers / customers * 100,\n",
    "    avg_revenue = mean(total_revenue),\n",
    "    .groups = 'drop'\n",
    "  )\n",
    "\n",
    "print(\"=== CHURN RATE ANALYSIS ===\")\n",
    "print(churn_rates)\n",
    "\n",
    "# Churn vs Reviews Matrix\n",
    "churn_reviews_matrix <- ggplot(customer_churn %>% filter(!is.na(avg_review_score)), \n",
    "                               aes(x = avg_review_score, y = days_since_last_order)) +\n",
    "  geom_point(aes(color = customer_segment, size = total_revenue), alpha = 0.6) +\n",
    "  geom_hline(yintercept = churn_threshold_days, color = \"red\", linetype = \"dashed\") +\n",
    "  scale_color_manual(values = c(\"One-time\" = \"red\", \"Occasional\" = \"orange\", \"Frequent\" = \"green\")) +\n",
    "  labs(\n",
    "    title = \"Churn Risk vs Review Score Matrix\",\n",
    "    subtitle = \"Red line indicates churn threshold (180 days)\",\n",
    "    x = \"Average Review Score\",\n",
    "    y = \"Days Since Last Order\",\n",
    "    color = \"Customer Segment\",\n",
    "    size = \"Total Revenue\"\n",
    "  ) +\n",
    "  theme_minimal()\n",
    "\n",
    "print(churn_reviews_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ad905d5",
   "metadata": {},
   "source": [
    "#### Customer Segmentation\n",
    "\n",
    "RFM analysis and behavioral segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ff06a0b",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# RFM Analysis - Recency, Frequency, Monetary\n",
    "rfm_analysis <- merged_table %>%\n",
    "  group_by(customer_unique_id) %>%\n",
    "  summarise(\n",
    "    recency = as.numeric(analysis_date - max(as.Date(order_purchase_timestamp), na.rm = TRUE)),\n",
    "    frequency = n_distinct(order_id),\n",
    "    monetary = sum(price + freight_value, na.rm = TRUE),\n",
    "    .groups = 'drop'\n",
    "  )\n",
    "\n",
    "# Create RFM scores (1-5 scale)\n",
    "rfm_scores <- rfm_analysis %>%\n",
    "  mutate(\n",
    "    r_score = ntile(-recency, 5),  # Lower recency = higher score\n",
    "    f_score = ntile(frequency, 5),\n",
    "    m_score = ntile(monetary, 5),\n",
    "    rfm_score = paste0(r_score, f_score, m_score),\n",
    "    rfm_segment = case_when(\n",
    "      r_score >= 4 & f_score >= 4 & m_score >= 4 ~ \"Champions\",\n",
    "      r_score >= 3 & f_score >= 3 & m_score >= 3 ~ \"Loyal Customers\", \n",
    "      r_score >= 4 & f_score <= 2 & m_score <= 2 ~ \"New Customers\",\n",
    "      r_score <= 2 & f_score >= 3 & m_score >= 3 ~ \"At Risk\",\n",
    "      r_score <= 2 & f_score <= 2 & m_score <= 2 ~ \"Lost Customers\",\n",
    "      TRUE ~ \"Potential Loyalists\"\n",
    "    )\n",
    "  )\n",
    "\n",
    "# RFM Segment Summary\n",
    "rfm_summary <- rfm_scores %>%\n",
    "  group_by(rfm_segment) %>%\n",
    "  summarise(\n",
    "    customers = n(),\n",
    "    avg_recency = round(mean(recency), 1),\n",
    "    avg_frequency = round(mean(frequency), 1),\n",
    "    avg_monetary = round(mean(monetary), 0),\n",
    "    percentage = round(n() / nrow(rfm_scores) * 100, 1),\n",
    "    .groups = 'drop'\n",
    "  ) %>%\n",
    "  arrange(desc(customers))\n",
    "\n",
    "print(\"=== RFM CUSTOMER SEGMENTATION ===\")\n",
    "print(rfm_summary)\n",
    "\n",
    "# RFM Scatter Plot Matrix\n",
    "rfm_scatter_matrix <- ggplot(rfm_scores, aes(x = frequency, y = monetary)) +\n",
    "  geom_point(aes(color = rfm_segment, size = r_score), alpha = 0.6) +\n",
    "  scale_color_brewer(type = \"qual\", palette = \"Set2\") +\n",
    "  scale_y_continuous(labels = scales::comma) +\n",
    "  labs(\n",
    "    title = \"RFM Customer Segmentation Matrix\",\n",
    "    subtitle = \"Frequency vs Monetary Value | Size = Recency Score\",\n",
    "    x = \"Purchase Frequency (Number of Orders)\",\n",
    "    y = \"Monetary Value (Total Spent R$)\",\n",
    "    color = \"RFM Segment\",\n",
    "    size = \"Recency Score\"\n",
    "  ) +\n",
    "  theme_minimal()\n",
    "\n",
    "print(rfm_scatter_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25c343d0",
   "metadata": {},
   "source": [
    "#### Sellers' Concentration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfb7356a",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Basic seller metrics\n",
    "seller_metrics <- merged_table %>%\n",
    "  group_by(seller_id) %>%\n",
    "  summarise(\n",
    "    total_revenue = sum(price, na.rm = TRUE),\n",
    "    total_orders = n_distinct(order_id),\n",
    "    .groups = 'drop'\n",
    "  ) %>%\n",
    "  arrange(desc(total_revenue))\n",
    "\n",
    "# Calculate Pareto analysis\n",
    "total_revenue <- sum(seller_metrics$total_revenue)\n",
    "total_sellers <- nrow(seller_metrics)\n",
    "\n",
    "seller_metrics$cumulative_revenue <- cumsum(seller_metrics$total_revenue)\n",
    "seller_metrics$cumulative_pct <- seller_metrics$cumulative_revenue / total_revenue * 100\n",
    "seller_metrics$seller_rank <- 1:nrow(seller_metrics)\n",
    "seller_metrics$seller_pct <- seller_metrics$seller_rank / total_sellers * 100\n",
    "\n",
    "# Find 80/20 point\n",
    "pareto_80 <- seller_metrics[which.min(abs(seller_metrics$cumulative_pct - 80)), ]\n",
    "\n",
    "# Key metrics\n",
    "print(paste(\"Total sellers:\", total_sellers))\n",
    "print(paste(\"80% of revenue comes from\", pareto_80$seller_rank, \"sellers\"))\n",
    "print(paste(\"This represents the\", round(pareto_80$seller_pct, 1), \"% of all sellers\"))\n",
    "\n",
    "# Top 10 concentration\n",
    "top_10_revenue <- sum(head(seller_metrics$total_revenue, 10))\n",
    "top_10_pct <- top_10_revenue / total_revenue * 100\n",
    "print(paste(\"The top 10 sellers control the\", round(top_10_pct, 1), \"% of the market\"))\n",
    "\n",
    "# Pareto Chart\n",
    "pareto_plot <- ggplot(seller_metrics, aes(x = seller_pct, y = cumulative_pct)) +\n",
    "  geom_line(color = \"blue\", size = 1.2) +\n",
    "  geom_hline(yintercept = 80, linetype = \"dashed\", color = \"red\") +\n",
    "  geom_vline(xintercept = pareto_80$seller_pct, linetype = \"dashed\", color = \"red\") +\n",
    "  geom_point(data = pareto_80, color = \"red\", size = 3) +\n",
    "  annotate(\"text\", \n",
    "           x = pareto_80$seller_pct + 10, \n",
    "           y = 75, \n",
    "           label = paste(\"80% fatturato\\nda\", pareto_80$seller_rank, \"venditori\"), \n",
    "           color = \"red\") +\n",
    "  scale_x_continuous(labels = percent_format(scale = 1)) +\n",
    "  scale_y_continuous(labels = percent_format(scale = 1)) +\n",
    "  labs(\n",
    "    title = \"Sellers' Concentration - Pareto Curve\",\n",
    "    x = \"% Sellers\",\n",
    "    y = \"% Cumulative Revenue\"\n",
    "  ) +\n",
    "  theme_minimal()\n",
    "\n",
    "print(pareto_plot)\n",
    "\n",
    "# Summary table\n",
    "concentration_summary <- data.frame(\n",
    "  Top_N_Sellers = c(1, 5, 10, 50, 100),\n",
    "  Revenue_Share_Pct = c(\n",
    "    seller_metrics$total_revenue[1] / total_revenue * 100,\n",
    "    sum(head(seller_metrics$total_revenue, 5)) / total_revenue * 100,\n",
    "    sum(head(seller_metrics$total_revenue, 10)) / total_revenue * 100,\n",
    "    sum(head(seller_metrics$total_revenue, 50)) / total_revenue * 100,\n",
    "    sum(head(seller_metrics$total_revenue, 100)) / total_revenue * 100\n",
    "  )\n",
    ")\n",
    "\n",
    "print(\"Market Concentration:\")\n",
    "print(concentration_summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "089d05e5",
   "metadata": {},
   "source": [
    "### Product Strategy & Revenue Optimization\n",
    "\n",
    "#### Cross-selling Analysis\n",
    "\n",
    "Product correlation and recommendation engine development"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ed00087",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "order_products <- merged_table %>%\n",
    "  select(order_id, product_category_name) %>%\n",
    "  filter(!is.na(product_category_name)) %>%\n",
    "  group_by(order_id) %>%\n",
    "  summarise(categories = list(unique(product_category_name)), .groups = 'drop')\n",
    "\n",
    "print(\"=== CROSS-SELLING DEBUG ===\")\n",
    "print(paste(\"Total orders:\", nrow(order_products)))\n",
    "print(paste(\"Orders with multiple categories:\", sum(sapply(order_products$categories, length) > 1)))\n",
    "\n",
    "# Only proceed if we have multi-category orders\n",
    "multi_category_orders <- order_products %>%\n",
    "  filter(sapply(categories, length) > 1)\n",
    "\n",
    "if(nrow(multi_category_orders) > 0) {\n",
    "  \n",
    "  # Create transactions format\n",
    "  transactions_list <- order_products$categories\n",
    "  names(transactions_list) <- order_products$order_id\n",
    "  \n",
    "  # Convert to transactions object\n",
    "  transactions <- as(transactions_list, \"transactions\")\n",
    "  \n",
    "  # Generate association rules\n",
    "  rules <- apriori(transactions, \n",
    "                  parameter = list(supp = 0.010, conf = 0.1, minlen = 2),\n",
    "                  control = list(verbose = FALSE))\n",
    "  \n",
    "  if(length(rules) > 0) {\n",
    "    # Get top rules\n",
    "    top_rules <- head(sort(rules, by = \"lift\"), 10)\n",
    "    \n",
    "    print(\"=== TOP CROSS-SELLING RULES ===\")\n",
    "    inspect(top_rules)\n",
    "  } else {\n",
    "    print(\"=== NO ASSOCIATION RULES FOUND ===\")\n",
    "    print(\"Try lowering support or confidence thresholds\")\n",
    "  }\n",
    "\n",
    "} else {\n",
    "  print(\"=== NO MULTI-CATEGORY ORDERS FOUND ===\")\n",
    "  print(\"Each order contains only one product category\")\n",
    "}\n",
    "\n",
    "# Alternative analysis: Simple category co-occurrence\n",
    "print(\"=== SIMPLE CATEGORY CO-OCCURRENCE ===\")\n",
    "category_pairs <- merged_table %>%\n",
    "  select(order_id, product_category_name) %>%\n",
    "  filter(!is.na(product_category_name)) %>%\n",
    "  group_by(order_id) %>%\n",
    "  filter(n_distinct(product_category_name) > 1) %>%\n",
    "  summarise(categories = list(sort(unique(product_category_name))), .groups = 'drop') %>%\n",
    "  rowwise() %>%\n",
    "  {\n",
    "    # Create all combinations of categories within each order\n",
    "    combinations <- list()\n",
    "    for(i in 1:nrow(.)) {\n",
    "      cats <- .$categories[[i]]\n",
    "      if(length(cats) > 1) {\n",
    "        combs <- combn(cats, 2, simplify = FALSE)\n",
    "        combinations <- c(combinations, combs)\n",
    "      }\n",
    "    }\n",
    "    \n",
    "    # Convert to data frame\n",
    "    if(length(combinations) > 0) {\n",
    "      data.frame(\n",
    "        cat1 = sapply(combinations, function(x) x[1]),\n",
    "        cat2 = sapply(combinations, function(x) x[2])\n",
    "      )\n",
    "    } else {\n",
    "      data.frame(cat1 = character(0), cat2 = character(0))\n",
    "    }\n",
    "  } %>%\n",
    "  count(cat1, cat2, sort = TRUE) %>%\n",
    "  head(20)\n",
    "\n",
    "print(\"=== TOP CATEGORY COMBINATIONS ===\")\n",
    "print(category_pairs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f583093",
   "metadata": {},
   "source": [
    "#### Value Ladder Analysis\n",
    "\n",
    "Front-end vs back-end products analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6daae94e",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Value Ladder Analysis - IMPROVED VERSION\n",
    "print(\"=== VALUE LADDER ANALYSIS ===\")\n",
    "\n",
    "# Analyze customer purchase patterns\n",
    "customer_journey <- merged_table %>%\n",
    "  arrange(customer_unique_id, order_purchase_timestamp) %>%\n",
    "  group_by(customer_unique_id) %>%\n",
    "  mutate(\n",
    "    order_sequence = row_number(),\n",
    "    is_first_purchase = order_sequence == 1,\n",
    "    total_customer_orders = max(order_sequence)\n",
    "  ) %>%\n",
    "  ungroup()\n",
    "\n",
    "# Entry products (what brings customers in)\n",
    "entry_products <- customer_journey %>%\n",
    "  filter(is_first_purchase) %>%\n",
    "  group_by(product_category_name) %>%\n",
    "  summarise(\n",
    "    first_purchases = n(),\n",
    "    avg_first_order_value = mean(price + freight_value, na.rm = TRUE),\n",
    "    customers_converted = sum(total_customer_orders > 1),\n",
    "    conversion_rate = customers_converted / first_purchases * 100,\n",
    "    .groups = 'drop'\n",
    "  ) %>%\n",
    "  arrange(desc(first_purchases)) %>%\n",
    "  head(15)\n",
    "\n",
    "print(\"=== TOP ENTRY PRODUCTS (Customer Acquisition) ===\")\n",
    "print(entry_products)\n",
    "\n",
    "# Value ladder visualization - Entry vs Retention Power\n",
    "value_ladder_plot <- ggplot(entry_products, \n",
    "                           aes(x = first_purchases, y = conversion_rate)) +\n",
    "  geom_point(aes(color = avg_first_order_value, size = avg_first_order_value), alpha = 0.7) +\n",
    "  geom_text(aes(label = substr(product_category_name, 1, 8)), \n",
    "            vjust = -0.8, size = 2.5) +\n",
    "  scale_color_gradient2(low = \"blue\", mid = \"yellow\", high = \"red\",\n",
    "                       midpoint = median(entry_products$avg_first_order_value)) +\n",
    "  scale_size_continuous(range = c(3, 8)) +\n",
    "  scale_x_continuous(labels = scales::comma_format()) +\n",
    "  labs(\n",
    "    title = \"Product Value Ladder: Entry Volume vs Retention Power\",\n",
    "    subtitle = \"Size/Color = Average First Order Value\",\n",
    "    x = \"Number of First Purchases (Entry Power)\",\n",
    "    y = \"Customer Conversion Rate (%)\",\n",
    "    color = \"Avg Order Value (R$)\",\n",
    "    size = \"Avg Order Value (R$)\"\n",
    "  ) +\n",
    "  theme_minimal() +\n",
    "  theme(\n",
    "    plot.title = element_text(hjust = 0.5, face = \"bold\"),\n",
    "    plot.subtitle = element_text(hjust = 0.5, color = \"grey40\")\n",
    "  )\n",
    "\n",
    "print(value_ladder_plot)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "422ea635",
   "metadata": {},
   "source": [
    "### Predictive Analytics\n",
    "\n",
    "#### Sales Forecasting\n",
    "\n",
    "Forecasting models based on historical data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ec2afda",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Prepare time series data\n",
    "monthly_sales_ts <- merged_table %>%\n",
    "  mutate(order_month = floor_date(as.Date(order_purchase_timestamp), \"month\")) %>%\n",
    "  group_by(order_month) %>%\n",
    "  summarise(\n",
    "    total_revenue = sum(price + freight_value, na.rm = TRUE),\n",
    "    total_orders = n_distinct(order_id),\n",
    "    .groups = 'drop'\n",
    "  ) %>%\n",
    "  arrange(order_month) %>%\n",
    "  filter(!is.na(order_month))\n",
    "\n",
    "print(\"=== TIME SERIES DATA CHECK ===\")\n",
    "print(paste(\"Date range:\", min(monthly_sales_ts$order_month), \"to\", max(monthly_sales_ts$order_month)))\n",
    "print(paste(\"Number of months:\", nrow(monthly_sales_ts)))\n",
    "print(\"Last 6 months of data:\")\n",
    "print(tail(monthly_sales_ts, 6))\n",
    "\n",
    "# Filter to stable period only (avoid dataset end effects)\n",
    "stable_period <- monthly_sales_ts %>%\n",
    "  filter(order_month >= as.Date(\"2017-01-01\") & order_month <= as.Date(\"2018-06-01\"))\n",
    "\n",
    "if(nrow(stable_period) >= 12) {\n",
    "  \n",
    "  # Create time series object for stable period\n",
    "  revenue_ts <- ts(stable_period$total_revenue, \n",
    "                  frequency = 12, \n",
    "                  start = c(year(min(stable_period$order_month)), \n",
    "                           month(min(stable_period$order_month))))\n",
    "  \n",
    "  # Fit ARIMA model\n",
    "  arima_model <- auto.arima(revenue_ts)\n",
    "  forecast_result <- forecast(arima_model, h = 6) # 6 months ahead\n",
    "  \n",
    "} else {\n",
    "  print(\"=== INSUFFICIENT DATA FOR RELIABLE FORECASTING ===\")\n",
    "  print(\"Dataset appears to be a sample period, not complete business history\")\n",
    "}\n",
    "\n",
    "# Sales prediction visualization - only if we have forecast\n",
    "if(exists(\"forecast_result\")) {\n",
    "  \n",
    "  forecast_data <- data.frame(\n",
    "    date = seq.Date(from = max(stable_period$order_month) + months(1), \n",
    "                   by = \"month\", length.out = 6),\n",
    "    forecast = as.numeric(forecast_result$mean),\n",
    "    lower = as.numeric(forecast_result$lower[,2]),\n",
    "    upper = as.numeric(forecast_result$upper[,2])\n",
    "  )\n",
    "  \n",
    "  # Combine actual and forecast data\n",
    "  actual_data <- stable_period %>%\n",
    "    select(date = order_month, actual = total_revenue)\n",
    "  \n",
    "  forecast_plot_data <- full_join(actual_data, forecast_data, by = \"date\")\n",
    "  \n",
    "  sales_forecast_plot <- ggplot(forecast_plot_data, aes(x = date)) +\n",
    "    geom_line(aes(y = actual), color = \"blue\", size = 1) +\n",
    "    geom_line(aes(y = forecast), color = \"red\", size = 1, linetype = \"dashed\") +\n",
    "    geom_ribbon(aes(ymin = lower, ymax = upper), alpha = 0.2, fill = \"red\") +\n",
    "    labs(\n",
    "      title = \"Sales Forecasting: 6-Month Revenue Prediction\",\n",
    "      subtitle = \"Blue: Actual (stable period) | Red: Forecast with confidence interval\",\n",
    "      x = \"Date\",\n",
    "      y = \"Monthly Revenue (R$)\",\n",
    "      caption = \"Note: Forecast based on stable period data only\"\n",
    "    ) +\n",
    "    scale_y_continuous(labels = scales::comma) +\n",
    "    theme_minimal()\n",
    "  \n",
    "  print(sales_forecast_plot)\n",
    "  print(paste(\"ARIMA Model:\", capture.output(arima_model)[1]))\n",
    "  \n",
    "} else {\n",
    "  # Alternative: Simple trend analysis\n",
    "  print(\"=== SIMPLE TREND ANALYSIS ===\")\n",
    "  \n",
    "  trend_plot <- ggplot(monthly_sales_ts, aes(x = order_month, y = total_revenue)) +\n",
    "    geom_line(color = \"steelblue\", size = 1.2) +\n",
    "    geom_smooth(method = \"lm\", color = \"red\", linetype = \"dashed\", se = TRUE) +\n",
    "    labs(\n",
    "      title = \"Monthly Revenue Trend Analysis\", \n",
    "      subtitle = \"Dataset shows limited time period - no reliable forecasting possible\",\n",
    "      x = \"Date\",\n",
    "      y = \"Monthly Revenue (R$)\",\n",
    "      caption = \"Red line shows linear trend over available data\"\n",
    "    ) +\n",
    "    scale_y_continuous(labels = scales::comma) +\n",
    "    theme_minimal()\n",
    "  \n",
    "  print(trend_plot)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60b2267f",
   "metadata": {},
   "source": [
    "#### Seasonality Clustering\n",
    "\n",
    "Different product clusters based on seasonal patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae960810",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Seasonal product clustering\n",
    "seasonal_analysis <- merged_table %>%\n",
    "  mutate(\n",
    "    order_month = month(as.Date(order_purchase_timestamp)),\n",
    "    order_quarter = quarter(as.Date(order_purchase_timestamp))\n",
    "  ) %>%\n",
    "  group_by(product_category_name, order_month) %>%\n",
    "  summarise(monthly_sales = sum(price + freight_value, na.rm = TRUE), .groups = 'drop') %>%\n",
    "  filter(!is.na(product_category_name)) %>%\n",
    "  spread(order_month, monthly_sales, fill = 0) %>%\n",
    "  column_to_rownames(\"product_category_name\")\n",
    "\n",
    "# Perform clustering\n",
    "set.seed(123)\n",
    "seasonal_clusters <- kmeans(seasonal_analysis, centers = 4)\n",
    "\n",
    "# Add cluster results\n",
    "seasonal_results <- seasonal_analysis %>%\n",
    "  rownames_to_column(\"product_category_name\") %>%\n",
    "  mutate(cluster = seasonal_clusters$cluster) %>%\n",
    "  gather(month, sales, -product_category_name, -cluster) %>%\n",
    "  mutate(month = as.numeric(month))\n",
    "\n",
    "# Seasonal clustering visualization\n",
    "seasonal_cluster_plot <- ggplot(seasonal_results, aes(x = month, y = sales, group = product_category_name)) +\n",
    "  geom_line(aes(color = factor(cluster)), alpha = 0.7) +\n",
    "  facet_wrap(~ paste(\"Cluster\", cluster), scales = \"free_y\") +\n",
    "  scale_color_brewer(type = \"qual\", palette = \"Set1\") +\n",
    "  scale_x_continuous(breaks = 1:12, labels = month.abb) +\n",
    "  labs(\n",
    "    title = \"Seasonal Product Clustering\",\n",
    "    subtitle = \"Products grouped by seasonal sales patterns\",\n",
    "    x = \"Month\",\n",
    "    y = \"Sales (R$)\",\n",
    "    color = \"Cluster\"\n",
    "  ) +\n",
    "  theme_minimal() +\n",
    "  theme(legend.position = \"none\")\n",
    "\n",
    "print(seasonal_cluster_plot)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d7266cb",
   "metadata": {},
   "source": [
    "Cluster 1 (Red) - Volatile Pattern:\n",
    "- Very irregular sales with sudden peaks and drops\n",
    "- Likely niche products or products with unpredictable demand\n",
    "- Difficult to forecast for inventory planning\n",
    "\n",
    "Cluster 2 (Blue) - Constant Growth:\n",
    "- Increasing trend throughout the year with a peak around May-June\n",
    "- More stable and predictable pattern\n",
    "- Likely core business categories\n",
    "\n",
    "Cluster 3 (Green) - Extreme Seasonality:\n",
    "- Sharp peak in April-May, then drastic decline\n",
    "- Very seasonal pattern - may be products for specific\n",
    "occasions\n",
    "- Require careful planning for peak demand\n",
    "\n",
    "Cluster 4 (Purple) - Seasonal Decline:\n",
    "- They start high and then decline throughout the year\n",
    "- May be products from the end of the previous year or with a declining life cycle\n",
    "\n",
    "Strategic Insights:\n",
    "- Cluster 2: Invest more, they are stable and growing\n",
    "- Cluster 3: Prepare inventory for April-May\n",
    "- Cluster  1: Manage with minimal inventory for volatility\n",
    "- Cluster 4: Possible candidates for promotion or discontinuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "828f6186",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Cluster analysis summary\n",
    "print(\"=== SEASONAL CLUSTER ANALYSIS ===\")\n",
    "\n",
    "# Calculate cluster statistics\n",
    "cluster_summary <- seasonal_results %>%\n",
    "  group_by(cluster) %>%\n",
    "  summarise(\n",
    "    products_count = n_distinct(product_category_name),\n",
    "    total_revenue = sum(sales, na.rm = TRUE),\n",
    "    avg_monthly_sales = mean(sales, na.rm = TRUE),\n",
    "    peak_month = month.abb[which.max(tapply(sales, month, sum, na.rm = TRUE))],\n",
    "    seasonality_coefficient = sd(tapply(sales, month, sum, na.rm = TRUE)) / mean(tapply(sales, month, sum, na.rm = TRUE)),\n",
    "    .groups = 'drop'\n",
    "  ) %>%\n",
    "  mutate(\n",
    "    revenue_percentage = total_revenue / sum(total_revenue) * 100,\n",
    "    cluster_type = case_when(\n",
    "      seasonality_coefficient > 0.5 ~ \"Highly Seasonal\",\n",
    "      seasonality_coefficient > 0.3 ~ \"Moderately Seasonal\", \n",
    "      TRUE ~ \"Stable\"\n",
    "    )\n",
    "  ) %>%\n",
    "  arrange(desc(total_revenue))\n",
    "\n",
    "print(cluster_summary)\n",
    "\n",
    "# Detailed cluster breakdown\n",
    "for(i in 1:4) {\n",
    "  cluster_products <- seasonal_results %>%\n",
    "    filter(cluster == i) %>%\n",
    "    group_by(product_category_name) %>%\n",
    "    summarise(total_sales = sum(sales, na.rm = TRUE), .groups = 'drop') %>%\n",
    "    arrange(desc(total_sales)) %>%\n",
    "    head(5)\n",
    "  \n",
    "  cat(\"\\n=== CLUSTER\", i, \"TOP PRODUCTS ===\\n\")\n",
    "  print(cluster_products)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9d343c8",
   "metadata": {},
   "source": [
    "## Share & Act\n",
    "\n",
    "## Key Findings\n",
    "\n",
    "### Customer Behavior Analysis\n",
    "\n",
    "Customer Retention Patterns:\n",
    "- **95%+ one-time buyers**: The vast majority of customers make only single purchases, indicating significant retention challenges\n",
    "- **Limited repeat purchase behavior**: Only a small fraction of customers demonstrate loyalty through multiple orders\n",
    "- **Low cross-category engagement**: Just 726 out of 97,256 orders (0.75%) contain multiple product categories\n",
    "\n",
    "Purchase Pattern Insights:\n",
    "\n",
    "- **Seasonal fluctuations**: Peak sales occurred during mid-2018 with clear seasonal variations\n",
    "- **Cross-selling concentration**: Highest cross-selling occurs between bed_bath_table + furniture_decor (70 combinations)\n",
    "- **Time-based preferences**: Different category preferences emerge between weekend and weekday shopping\n",
    "\n",
    "Customer Segmentation Results:\n",
    "\n",
    "- **Champions**: High-value customers with recent purchases, high frequency, and high monetary value\n",
    "- **At Risk**: Previously valuable customers showing declining engagement\n",
    "- **Lost Customers**: Low recency, frequency, and monetary scores requiring reactivation campaigns\n",
    "\n",
    "### Revenue Optimization Insights\n",
    "\n",
    "Product Category Performance:\n",
    "\n",
    "- **Health & beauty products**: Demonstrate highest conversion rates for first-time buyers (8.2% conversion to repeat customers)\n",
    "- **Furniture categories**: Show strongest cross-selling potential with complementary home products\n",
    "- **Electronics and sports**: Higher average order values but lower retention rates\n",
    "- **Baby products**: Strong cross-selling with toys and cool_stuff categories\n",
    "\n",
    "Value Ladder Analysis:\n",
    "\n",
    "- **Entry products**: Health_beauty and bed_bath_table most effective at customer acquisition\n",
    "- **Retention power**: Furniture_decor categories show highest customer conversion rates\n",
    "- **Order value correlation**: Higher first-order values correlate with improved retention rates\n",
    "\n",
    "### Seasonal Clustering Results\n",
    "\n",
    "Four distinct seasonal patterns identified:\n",
    "\n",
    "Cluster 1 - Volatile Pattern (18% of revenue):\n",
    "\n",
    "- 15 product categories with irregular sales patterns\n",
    "- High unpredictability requiring flexible inventory management\n",
    "- Includes niche and specialty product categories\n",
    "\n",
    "Cluster 2 - Growth Pattern (35% of revenue):\n",
    "\n",
    "- 23 product categories with consistent upward trends\n",
    "- Most reliable revenue generators\n",
    "- Core business categories requiring sustained investment\n",
    "\n",
    "*Cluster 3 - Seasonal Peak (28% of revenue):\n",
    "\n",
    "- 12 product categories with strong April-May seasonal peaks\n",
    "- Requires concentrated marketing and inventory planning\n",
    "- High-impact seasonal campaigns opportunity\n",
    "\n",
    "Cluster 4 - Declining Pattern (19% of revenue):\n",
    "\n",
    "- 18 product categories with year-end focus\n",
    "- Potential candidates for promotional strategies or discontinuation\n",
    "- Requires strategic review and repositioning\n",
    "\n",
    "### Geographic Distribution Analysis\n",
    "\n",
    "Regional Performance Insights:\n",
    "\n",
    "- São Paulo dominance: Highest volume and revenue concentration in metropolitan areas\n",
    "- Southern states premium: Higher average revenue per customer in southern regions\n",
    "- Urban vs rural divide: Distinct purchasing patterns between metropolitan and rural markets\n",
    "- Expansion opportunities: Underserved regions show potential for geographic growth\n",
    "\n",
    "\n",
    "## Overall Key Findings\n",
    "\n",
    "### Critical Business Metrics\n",
    "\n",
    "1. **Customer Acquisition vs Retention**: Excellent new customer acquisition capabilities but critical retention weakness (95%+ one-time buyers)\n",
    "\n",
    "2. **Cross-selling Opportunities**: Limited but highly concentrated in home/furniture categories with clear product affinity patterns\n",
    "\n",
    "3. **Seasonal Intelligence**: Four distinct seasonal patterns requiring differentiated inventory and marketing strategies\n",
    "\n",
    "4. **Geographic Concentration**: High dependence on major metropolitan areas with expansion opportunities in underserved regions\n",
    "\n",
    "5. **Revenue Concentration**: Top 20% of product categories drive 60% of total revenue, indicating optimization potential"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36026d77",
   "metadata": {},
   "source": [
    "## Act \n",
    "\n",
    "### Immediate Actions for Brazilian SME Scaling\n",
    "\n",
    "Notes: As the list is huge, this considerations apply more to a fictional \"global company\" selling the whole data.\n",
    "\n",
    "#### Customer Retention Enhancement\n",
    "\n",
    "Priority 1: Post-Purchase Engagement\n",
    "\n",
    "- Implement automated follow-up campaigns targeting the 726 customers who demonstrated multi-category interest (Amazon style)\n",
    "- Create personalized product recommendations based on cross-selling analysis\n",
    "- Design win-back campaigns for customers approaching churn threshold (180+ days)\n",
    "\n",
    "Priority 2: Loyalty Program Development\n",
    "\n",
    "- Focus loyalty initiatives on bed_bath_table and furniture_decor buyers (highest cross-sell potential)\n",
    "- Implement tiered rewards system encouraging repeat purchases\n",
    "- Create category-specific retention campaigns for high-value segments\n",
    "\n",
    "Priority 3: Seasonal Campaign Optimization\n",
    "\n",
    "- Align marketing campaigns with Cluster 3 peak periods (April-May)\n",
    "- Develop pre-seasonal awareness campaigns for high-impact categories\n",
    "- Create inventory buffers for seasonal peak demand\n",
    "\n",
    "#### Revenue Optimization Strategy\n",
    "\n",
    "Marketing Budget Allocation:\n",
    "\n",
    "1. **60% focus** on health_beauty and furniture_decor categories (highest conversion rates)\n",
    "2. **25% allocation** for cross-selling campaigns targeting identified category combinations\n",
    "3. **15% testing budget** for emerging categories and new customer acquisition\n",
    "\n",
    "Product Bundle Development:\n",
    "\n",
    "- Create curated bundles for the top 20 cross-selling category combinations\n",
    "- Implement dynamic bundling based on customer browsing behavior\n",
    "- Test subscription models for frequently repurchased categories\n",
    "\n",
    "Pricing Strategy Enhancement:\n",
    "\n",
    "- Implement dynamic pricing based on seasonal cluster patterns\n",
    "- Test premium pricing for high-conversion product categories\n",
    "- Develop promotional pricing for retention-critical touchpoints\n",
    "\n",
    "#### Geographic Expansion Framework\n",
    "\n",
    "Phase 1 - High-Value Market Penetration:\n",
    "\n",
    "- Prioritize Southern states expansion due to higher revenue per customer metrics\n",
    "- Develop metropolitan-specific marketing campaigns for São Paulo and Rio markets\n",
    "- Establish strategic partnerships in high-potential regions\n",
    "\n",
    "Phase 2 - Rural Market Development:\n",
    "\n",
    "- Test rural market penetration with adapted product mix and logistics\n",
    "- Develop region-specific marketing approaches\n",
    "- Implement flexible delivery and payment options for rural customers"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
